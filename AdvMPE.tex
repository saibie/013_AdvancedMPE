%% LyX 1.6.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[10pt,a4paper,english]{amsart}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
%\usepackage{endnotes}
\usepackage{units}
%\usepackage{multirow}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{cite}
%\usepackage{natbib}
\usepackage{amsthm}
\usepackage{array,arydshln}
\usepackage[pdftex]{graphicx}
\usepackage{rotating}
\usepackage{ifpdf}
%\usepackage{epsfig}
\usepackage[all]{xy}
\usepackage{latexsym}
\usepackage[hidelinks]{hyperref}
\usepackage{color}
\usepackage{eucal}
\usepackage{mathrsfs}
%\usepackage{hfont}


\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section} %% Comment out for sequentially-numbered
\numberwithin{figure}{section} %% Comment out for sequentially-numbered
\numberwithin{table}{section}
\let\footnote=\endnote
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{theorem}[thm]{Theorem}
\newtheorem{Theorem}[thm]{Theorem}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{Def}[thm]{Definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{exam}[thm]{Example}
\newtheorem{algo}[thm]{Algorithm}
%  \theoremstyle{plain}
\newtheorem{assumption}[thm]{Assumption}
\theoremstyle{plain}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{lemma}[thm]{Lemma}
\theoremstyle{plain}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{corollary}[thm]{Corollary}
\theoremstyle{plain}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{rem}[thm]{Remark}

\def\norm#1{\|#1\|}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%\newcommand{\norm}[1]{\|#1\|}
\def\norm#1{\|#1\|}
\def\normm#1#2{\|#1\|_{#2}}
\def\normF#1{\|#1\|_{F}}
\def\Proof{{\bf Proof.\enspace}}
\def\vec{{\sf vec}}
%\def\unvec{\mathrm{unvec}}
\def\tr{\mathrm{tr}}
%\def\tr{\textrm{tr}}
\def\bmatrix#1{\left[\begin{matrix}#1\end{matrix}\right]}
\def\pmatrix#1{\left(\begin{matrix}#1\end{matrix}\right)}
\def\RR{\mathbb{R}}
\def\NN{\mathbb{N}}
\def\CC{\mathbb{C}}
\def\nbyn{n\times n}
\def\mbyn{m\times n}
\def\mbym{m\times m}
\def\pbyq{p\times q}
\def\nnbynn{n^{2}\times n^{2}}
\def\mbf#1{\mathbf{#1}}
\def\mrm#1{\mathrm{#1}}
\def\bpi{\boldsymbol{\pi}}
\def\a{\alpha}
\def\b{\beta}
\def\d{\delta}
\def\e{\varepsilon}
\def\l{\lambda}
\def\D{\mathcal{D}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\Q{\mathcal{Q}}
\def\M{\mathcal{M}}
\def\N{\mathcal{N}}
\def\P{\mathcal{P}}
\def\X{\mathcal{X}}
\def\proj{\mathbf{P}}
\def\pjn{\mathbf{P}_{\mathcal{N}}}
\def\pjm{\mathbf{P}_{\mathcal{M}}}
\def\tXi{\tilde{X}_{i}}
\def\tXii{\tilde{X}_{i+1}}
\def\dpm#1{\begin{displaymath}#1\end{displaymath}}
\def\bdm{\begin{displaymath}}
\def\edm{\end{displaymath}}
\def\dtyl{\displaystyle}
\def\ones#1{\mathbf{1}_{#1}}
%\def\onesn{\mathbf{1}_{n \times n}}

\definecolor{gray}{rgb}{.5,.5,.5}

\makeatother

\begin{document}
	
\title[Solving Nearly Non-simple MPE by Newton's Method with Line Searches]{Solving Nearly Non-simple Matrix Polynomial Equations by Newton's Method with Advanced Line Searches}
\author{Sang-hyup Seo}
\address{Sang-hyup Seo\\Where}
\email{saibie1677@gmail.com}
\date{\today}

\begin{abstract}
%	We consider the Newton iteration for a matrix polynomial equation which arises in stochastic problem.
%	In this paper, we show that the elementwise minimal nonnegative solution of the matrix polynomial equation can be obtained using Newton's method if the equation satisfies the sufficient condition, and
%	the convergence rate of the iteration is quadratic if the solution is simple.
%	Moreover, we show that the convergence rate is at least linear if the solution is non-simple,
%	but we can apply a modified Newton method whose iteration number is less than the pure Newton iteration number.
%	Finally, we give a numerical experiment which is related with our issue.
\end{abstract}

\keywords{matrix polynomial equation, positive solution, nonnegative solution, $M$-matrix, Newton's method, line search, acceleration of a method, nearly non-simple}

\subjclass[2010]{65H10}

% \thanks{$\dagger$Corresponding Author}

\maketitle

\section{Introduction}\label{sec:intro}

We consider a matrix polynomial equation(MPE) with $n$-degree defined by
\begin{equation}\label{eq:MPE}
P(X)=\sum_{k=0}^{n} A_{k}X^{k}= A_{0}+A_{1}X+\cdots+A_{n-1}X^{n-1} + A_{n}X^{n}=0,
\end{equation}
where the coefficient matrices $A_{k}$'s are $\mbym$ matrices.
Then, the unknown matrix $X$ must be an $\mbym$ matrix.
A matrix $S$ is called a solution of \eqref{eq:MPE} if $P(S) = 0$.


The MPE \eqref{eq:MPE} often occurs in the theory of differential equations, system theory, network theory, stochastic theory, quasi-birth-and-death
and other areas \cite{ALFA2003, Bean1997, Butler1985, Gohberg1982, Lancaster1966, Lancaster1985, He2001, bini2005, Latouche1999}.
Specially, in quasi-birth-and-death and stochastic problems, finding the minimal nonnegative solution of a matrix equation is an important issue.

There are many researches to find the minimal nonnegative solution.
%Davis \cite{Davis1981, Davis1983} and Higham, Kim \cite{Higham2000, Higham2001} studied the Newton method for a quadratic matrix equation.
Guo and Laub \cite{Guo2000} considered a nonsymmetric algebraic Riccati equation, and they proposed iteration algorithms which converge to the minimal positive solution. 
In \cite{Guo2001}, Guo provided a sufficient condition for the existence of nonnegative solutions of nonsymmetric algebraic Riccati equations.
Kim \cite{Kim2008} showed that the minimal positive solutions also can be found by the Newton method with the zero initial matrices in some different types of quadratic equations.
%Hautphenne, Latouche, and Remiche \cite{SophieHautphenne2008} studied the Newton method for the Markovian binary tree.
Seo and Kim \cite{SeoSeoKim2013, SeoKim2014} studied the Newton iteration for matrix polynomial equations.

Newton's method is one of powerful tools to find solutions of nonlinear matrix equations.
By Kantorovich theorem \cite{Kantorovich1964}, the convergence rate of the method is quadratic if the derivative on the domain is Lipschitz continuous and at the solution is nonsingular.
But, if the derivative at the solution is singular, then we cannot apply Kantrovich theorem, i.e., we cannot guarantee that the rate is quadratic.
The followings are researches to analyze the problems with singular derivative at the solution and improve the method.

For general functions on Banach spaces, Reddien \cite{Reddien1978}, Decker and Kelley \cite{DeckerKelley1980Newton1, DeckerKelley1980Newton2} gave analyses about Newton's method for singular problems.
In \cite{DeckerKellerKelley1983}, Decker, Keller, and Kelley provided an acceleration of Newton's method for singular problems and analyzed for the convergence rate of the method.
Kelley and Suresh \cite{Kelley1983} suggested a new accelerated Newton's method at singular points.
Decker and Kelley \cite{DeckerKelley1985} showed an analysis for Newton's method
at nearly singular roots.
In \cite{Kelley1986}, Kelley analyzed convergence rate of the method for functions whose high order derivatives at the solution are singular.
For specific functions on $\RR^{\mbyn}$, Guo and Lancaster \cite{Guo1998M} analyzed and provided a modification about Newton's method for algebraic Riccati equations at singular roots.
In \cite{SeoSeo2020}, S-.H. Seo and J-.H. Seo suggested a modified Newton method for matrix polynomial equations with $n$-degree for singular problems.

In \cite{Kelley1983, DeckerKellerKelley1983}, accelerations for Newton's method at singular roots was suggested for general functions on Banach spaces.
For specific functions on $\RR^{\mbyn}$, modifications of Newton's method at singular roots was provided in \cite{Guo1998M, SeoSeoKim2018}.
But, the main hypotheses of the papers are similar and that the solution is non-simple, i.e., the derivative at the solution is singular.
It means that the accelerations for Newton's method cannot be applied and the accelerated iterations cannot be guaranteed converge to the solutions if the solution is simple.
Even if the iterations converge to the solutions, they cannot be guaranteed better than the pure iterations.


Otherwise, in \cite{DeckerKelley1985}, authors analyzed and suggested an acceleration for Newton's method about the case of nearly singular roots with the general bifurcation problem \cite{Keller1977, Stakgold1971}.
In \cite{SeoKim2014}, it was suggested that a global accelerated Newton's method, which can be applied whether the solution is simple or not, for \eqref{eq:MPE}.

The aim of this paper is also to provide an improved Newton's method for \eqref{eq:MPE} which satisfies following Assumption \ref{ass:MPE} at nearly singular roots.
We show that the differences between the Newton sequences and the solution lie on nearly one-dimensional space.
Moreover, with the exact line search idea in \cite{Seo2008}, we suggest a proper acceleration weight $\a$.

\begin{assumption}\label{ass:MPE}
	For the MPE \eqref{eq:MPE},
	\begin{enumerate}[1)]
		\item The coefficient matrices $A_{k}$'s are nonnegative except $A_{1}$.
		\item $-A_{1}$ is a nonsingular $M$-matrix.
		\item $A_{0}$, $A_{1}$, and $\sum_{k=2}^{n}A_{k}$ are irreducible.
	\end{enumerate}
\end{assumption}


Here, we give some basic definitions and lemmas for this paper.

Let $A, B \in \RR^{\mbym}$ be matrices.
If all elements of $A$ is nonnegative, then we call that $A$ is a {\it nonnegative} matrix and denote $A \geq 0$. In similar sense, we define $A \leq 0$, $A > 0$, and $A < 0$.
If a matrix $A$ can be written as $rI_{m}-B$ with $B \geq 0$ and $r \in \RR$, we call that $A$ is a \textit{$Z$-matrix}.
Moreover, if $A=rI_{m}-B$ is a $Z$-matrix and $r\geq\rho(B)$ then $A$ is called an $M$-matrix.
The following is a basic theorem for $M$-matrices.

\begin{thm}\label{thm:proMmat}%{\rm \cite[Theorem 2.1]{Guo2007}, \cite[Theorem 2.1]{Poole1974}}
	For a $Z$-matrix $A$, the following are equivalent:
	\begin{enumerate}
		\item $A$ is a nonsingular $M$-matrix.
		
		\item $A^{-1}$ is nonnegative.
		
		\item $Av>0$ for some vector $v>0$.
		
		\item All eigenvalues of $A$ have positive real parts.
	\end{enumerate}
\end{thm}

Let a function $F : \RR^{\mbyn} \rightarrow \RR^{\mbyn}$ and an equation $F(X) = 0$ be given.
If nonnegative solutions $S_{1}$ and $S_{2}$ of $F(X) = 0$ are satisfy
	\begin{equation}\label{eq:s1ss2}
	S_{1}\leq S\leq S_{2},\end{equation}
for any nonnegative solution $S$ of $F(X) = 0$, then they are called the {\it minimal nonnegative solution} and the {\it maximal nonnegative solution}, respectively.
The {\it minimal positive solution} and the {\it maximal positive solution} also are can be defined, similarly.
If the Fr\'echet derivative of $F$ at a solution $S$ is nonsingular, then $S$ is called {\it simple}.
Furthermore, we call that a not simple solution is {\it non-simple}.
In this paper, for convenience, the notation $||\cdot||$ is used instead of the Frobenius norm $||\cdot||_{F}$ and $\tilde{X}$ denotes $S - X$, the difference between $X$ and the solution $S$, 
because they are used very frequently.




\section{Convergence of Newton's Method for the MPE}\label{sec:analysis}

In this paper, we denote that
\begin{equation}\label{eq:DerivativeMPE}
P'[X](H)=\sum_{k=1}^{n} \sum_{l=0}^{k-1} A_{k}X^{l}HX^{k-l-1}
\end{equation}
is the Fr\'{e}chet derivative of the MPE \eqref{eq:MPE} at $X$ in the direction $H$, and
\begin{equation}\label{eq:2DerivativeMPE}
P''[X](K,H)=\sum_{k=2}^{n}\sum_{l=0}^{k-2}\sum_{j=0}^{l} A_{k} \left( X^{l}HX^{j}KX^{n-l-j-2} + X^{l}KX^{j}HX^{n-l-j-2} \right)
\end{equation}
is the second Fr\'{e}chet derivative at $X$.

For given initial guess $X_{0}$, the Newton iteration of \eqref{eq:MPE},
\begin{equation}\label{eq:NewtonStepMPE2}
X_{i+1}=X_{i}-P'[X_{i}]^{-1}(P(X_{i})),\quad i=0,1,2,\ldots
\end{equation}
can be separated into two parts as
\begin{equation}\label{eq:NewtonStepMPE}
\begin{cases}
P'[X_{i}](H_{i}) = -P(X_{i}),\\
X_{i+1}=X_{i}+H_{i},\end{cases}\quad i=0,1,2,\ldots.
\end{equation}
If we define that
\begin{equation}
	\P'[X] = \vec \circ P[X]^{\prime} \circ \vec^{-1} = \sum_{k=1}^{n} \sum_{l=0}^{k-1} (X^{k-l-1})^{T} \otimes A_{k}X^{l},
\end{equation}
then solving the above equation of \eqref{eq:NewtonStepMPE} is equivalent to solving the following $m^{2}\times m^{2}$ linear system
\begin{equation}
\P'[X_{i}]\vec(H_{i}) = \vec(-P(X_{i})),
\end{equation}
where $\vec$ is the vectorization by \cite[Lemma 4.3.1]{Horn1994}.



We introduce a sufficient condition of the existence of
the minimal nonnegative solution of the MPE \eqref{eq:MPE} with Assumption \ref{ass:MPE},
and give some analysis for Newton's method.

\begin{theorem}\label{thm:SuffMPE}{\rm \cite[Theorem 2.1]{Meng2017}}
	Let the MPE \eqref{eq:MPE} with 1) and 2) in Assumption \ref{ass:MPE} be given.
	Then, there exists the minimal nonnegative solution if
	\begin{equation}\label{eq:SuffMPE}
	-\sum_{k=0}^{n}A_{k}~\textrm{is a nonsingular or singular irreducible $M$-matrix.}
	\end{equation}
	
\end{theorem}




\begin{theorem}\label{thm:ConvergenceNewtonMPE}{\rm\cite[Theorem 2.2]{SeoSeoKim2018}}
	Suppose that the MPE \eqref{eq:MPE} satisfies Assumption \ref{ass:MPE} and \eqref{eq:SuffMPE}.
	Then, the Newton sequence $\{X_{i}\}$ with $X_{0} = 0$ is well defined, is monotone nondecreasing,
	and converges to the minimal positive solution $S$.
	Furthermore, $-\P'[X_{i}]$ is a nonsingular irreducible $M$-matrix for $i \in \NN$, and $-\P'[S]$ is an irreducible $M$-matrix.
\end{theorem}





\section{The Nearly Singular Derivative $-P'[S]$}\label{sec:nearlysingular}
Generally, by Kantorovich theorem \cite{Kantorovich1964}, the Newton iteration \eqref{eq:NewtonStepMPE2} converges quadratically if $-P'[S]$ is nonsingular.
But, if $-P'[S]$ becomes nearly singular, the quadratic convergence radius will be very small.
In this section, we will see the converging tendency of \eqref{eq:NewtonStepMPE2} when $-P'[S]$ is nearly singular.

Let the MPE \eqref{eq:MPE} satisfy Assumption \ref{ass:MPE}, $S$ be the minimal positive solution of \eqref{eq:MPE}.
Then, $-\P'[S]$ is an irreducible $M$-matrix.
Therefore, the eigenvalue $\mu$ of $-\P'[S]$ whose absolute value is the smallest in absolute values of all eigenvalues is nonnegative and simple by \cite[Perron-Frobenius Theorem]{Horn1985}.

Here, we think over an other MPE,
\begin{equation}\label{eq:epsMPE}
	P_{\e}(X) = (A_{0} - \e S) + (A_{1} + \e I_{m})X + \sum_{k=2}^{n}A_{k}X^{k} = 0,
\end{equation}
where $\e \leq \mu$.
%Then, $S$ is a solution of \eqref{eq:epsMPE}.
Since the representation of the Fr\'echet derivative of $P_{\e}$ at $S$ is that
\begin{align*}
\P'_{\e}[S] &= I_{m} \otimes (A_{1} + \e I_{m}) + \sum_{k=2}^{n}\sum_{l=0}^{k-1}(X^{k-l-1})^{T} \otimes A_{k}X^{l}\\
&= \P'[S] + \e I_{m^2},
\end{align*}
$-\P'_{\e}[S]$ is also an $M$-matrix by Theorem \ref{thm:proMmat}, and the nonnegative number $\mu - \e$ is the smallest eigenvalue of $-\P'_{\e}[S]$.

\begin{lem}\label{thm:PeConv2MinSol}
	For $\e \leq \mu$, \eqref{eq:epsMPE} has $S$ as the minimal positive solution.
\end{lem}
\begin{proof}
	Clearly, $S$ is a solution of \eqref{eq:epsMPE}.
	Here, we show that $S$ is the unique solution in $L[0,S] := \{ X \in \RR^{\mbym} : 0 \leq X \leq S \}$.
	Let $\{X_{\e, i}\}$ be the Newton sequence of \eqref{eq:epsMPE} with $X_{\e,0} = 0$.
	Then,
	\begin{equation}
		X_{\e,1} = (- A_{1} - \e I_{m})^{-1}(A_{0} - \e S)
	\end{equation}
\end{proof}


\section{What}
This paragraph is with reference to \cite{DeckerKelley1985}.
Let $F_{\e}(X) = 0$ be the general bifurcation problem where $\e$ is a chosen continuation parameter and $\e = 0$ denotes the bifurcation point and let $S_{\e}$ denote a solution arc through the branch point.
We assume that the follwings hold,
\begin{equation}\label{eq:bifurcationAssumption}
	\begin{array}{l}
		\N_{0}={\rm Ker}(-F'_{0}[S_{0}])={\rm span}\{E_{0}\},\quad\|E_{0}\| = 1, \\
		-F'[S_{0}](\M_{0}) = \M_{0},\qquad \RR^{\mbym} = \M_{0} \oplus \N_{0}.
	\end{array}	
\end{equation}
Then, we can define $\proj_{\N_{0}}$ to be the projection onto $\N_{0}$ parallel to $\M_{0}$ and $\proj_{\M_{0}}=I-\proj_{\N_{0}}$.
Clearly, $F'_{0}[S_{0}]$ is a Fredholm operator of index zero and hence will remain so for $\e$ near zero.
In this case, it can be shown \cite{CrandallRabinowitz1973} that the structure of \eqref{eq:bifurcationAssumption} is also preserved for small $\e$, i.e., we can solve
\begin{equation}
	-F'_{\e}[S_{\e}](E_{\e}) = \mu_{\e}E_{\e}
\end{equation}
for a continuous eigenvalue-eigenvector pair $(\mu_{\e}, E_{\e})$ where $\mu_{0} = 0$.

We assume that
\begin{equation}
	\begin{array}{l}
		-P'[S](E) = \e E, \quad\|E\| = 1, \quad\N_{\e}={\rm span}\{E\},\\
		\RR^{\mbym} = \M_{\e} \oplus \N_{\e},\qquad P'[S](\M_{\e}) = \M_{\e},
	\end{array}	
\end{equation}
where $\e$ is a simple eigenvalue of $-\P'[S]$ whose absolute value is the smallest.
Then, we can define $\proj_{\N_{\e}}$ to be the projection onto $\N$
parallel to $\M$ and $\proj_{\M_{\e}}=I-\proj_{\N_{\e}}$.
If $\e = 0$, then the following theorem shows the converging tendency of \eqref{eq:NewtonStepMPE2} for the singular $-P'_{0}[S]$.


\begin{theorem}{\rm (cf. \cite[Theorem 1.5]{Guo1998M}, \cite[Theorem 1.1]{Kelley1986})}\label{thm:SingularConvergence}
	Let $\proj_{\N_{0}} P''[S](E, E)$ be nonzero and let
	\begin{equation}\label{eq:Wset}
		W(\rho, \theta) = \left\{ X \left|\,0 < \|\tilde{X}\| < \rho, ~\|\proj_{\M_{0}}(\tilde{X})\| \leq \theta \|\proj_{\N_{0}}(\tilde{X})\|\right.\right\}.
	\end{equation}
	If $X_{0}\in W(\rho_{0}, \theta_{0})$ for $\rho_{0},\theta_{0}$ sufficiently small, then the Newton sequence $\{X_{i}\}$ is well defined and $\| P'[X_{i}]^{-1}\|\leq c\|\tXi\|^{-1}$ for all $i\geq1$ and some constant $c>0$. Moreover,
	\begin{equation}
		\lim_{i \rightarrow \infty}\frac{\|\tXii\|}{\|\tXi\|} = \frac{1}{2},
		\qquad \lim_{i \rightarrow \infty}\frac{\|\proj_{\M_{0}}(\tXi)\|}{\|\proj_{\N_{0}}(\tXi)\|^{2}}=0.
	\end{equation}
\end{theorem}

Defining
\begin{align*}
	\l E &= \proj_{\N_{\e}} P''[S](E, E),\\
	\mu E &= \proj_{\N_{\e}} \tilde{X},\numberthis\\
	s &= {\rm sign}(\e\l),
\end{align*}
we consider the conical region
\begin{equation}\label{eq:Wsset}
	W_{s}(\rho, \theta) = \left\{ X \left|\,0 < \|\tilde{X}\| < \rho, ~\|\proj_{\M_{\e}}(\tilde{X})\| \leq \theta \|\proj_{\N_{\e}}(\tilde{X})\|, ~{\rm sign}(\mu) = s \right.\right\}.
\end{equation}

\begin{theorem}{\rm (cf. \cite[Theorem 3.22]{DeckerKelley1985})}\label{thm:NealySingularConvergence}
	Assume $\l \neq 0$, and $0 < \e < \bar{\e}$.
	Then, there are continuous functions $\rho = \rho(\e)$, $\theta = \theta(\e)$, monotonically increasing as $\e \downarrow 0$ such that if $X_{0} \in W_{s}(\rho, \theta)$ then $F_{X_{0}}^{\prime -1}$ exists and all subsequent Newton iterates remain in this set and converges to $S = S(\e)$. Further,
	\begin{equation}
		\|\pjn\tXii\| < \tfrac{3}{4}\|\pjn\tXi\|, \qquad \|\pjm(\tXii)\| \leq K\|\tXi\|^{2},
	\end{equation}
	for $i \in \NN \cup \{0\}$, some $K > 0$, and $\rho(\e)$, $\theta(\e)$ may be chosen such that $\rho(0) = \rho_{0}$, $\theta(0) = \theta_{0}$ where $\rho_{0}$, $\theta_{0}$ are values for which the conclusions of Theorem \ref{thm:SingularConvergence} hold.
\end{theorem}



\begin{lemma}\label{lem:BisInvertible}
	Suppose the matrix polynomial equation \eqref{eq:MPE} satisfies Assumption \ref{ass:MPE}.
	If the matrix $-\mathcal{P}'_{S}$ is a singular $M$-matrix,
	then $0$ is a simple eigenvalue of $-\mathcal{P}'_{S}$, $\N\oplus\M=\mathbb{R}^{m \times m}$,
	$\N$ is one-dimensional and the map $\mathcal{B}_{N_{0}}$ is invertible
	for some nonzero $N_{0}\in\N$.
\end{lemma}


\begin{proof}
	Since $S$ is positive and $A_{k}$'s are irreducible, $-\P'_{S}$ is irreducible.
	Then, by Perron-Frobenius Theorem \cite[Theorem 8.4.4]{Horn1985},
	$0$ is a simple eigenvalue of $\P'_{S}$ with a positive eigenvector. Thus, we can find $n^{2}$ linearly independent
	vectors $\chi_{1},\chi_{2},\cdots \chi_{n^{2}}$ such that $\chi_{1}>0$ and
	
	\begin{equation}\label{eq:invUDsU}
	\X^{-1}\P'_{S}\X=\bmatrix{
		0 & 0\\
		0 & \mathcal{D}}, ~\textrm{where }\X=\left[\begin{array}{c|c|c|c} \phantom{1}&\phantom{1}&\phantom{1}&\phantom{1}\\ \chi_{1}&\chi_{2}&\cdots&\chi_{n^{2}}\\\phantom{1}&\phantom{1}&\phantom{1}&\phantom{1}
	\end{array} \right]
	\end{equation}
	and $\mathcal{D}$
	is an $(n^{2}-1)\times(n^{2}-1)$ nonsingular matrix.
	By the same way, we also have a positive vector $\psi$ such that $\psi^{T}\P'_{S} = 0$.
	Now, $P'_{S}(N) = 0$ if and only if $\P'_{S}\mathrm{vec}(N)=0$.
	From \eqref{eq:invUDsU}, $\P'_{S}\mathrm{vec}(N) = 0$ if and only if $\mathrm{vec}(N) \in \mathrm{span}(\chi_{1})$,
	in which case we write $N = a\vec^{-1}(\chi_{1})$ for some nonzero $a \in \RR$.
	Thus, $\N = \mathrm{span}(\vec^{-1}(\chi_{1}))$.
	Simiarly, $\M = \mathrm{span}(\vec^{-1}(\chi_{2}), \ldots, \vec^{-1}(\chi_{n^{2}}))$.
	Therefore, $\N$ is one-dimensional and $\mathbb{R}^{m \times m}=\N\oplus\M$.
	
	To prove the map $\mathcal{B}_{N_{0}}$ is invertible for a nonzero matrix $N_{0} \in \N$ , we only need to show that
	$$
	\pjn\left( P''_{S}(N_{0},N) \right) \neq 0,
	$$
	for all nonzero $N \in \N$ because $\mathcal{B}_{N_{0}}$ is linear and $\N$ is one-dimensional.
	Since $\vec^{-1}(\chi_{1})>0$ and $S > 0$, we have
	\begin{align*}
	P''_{S}(N_{0},N) & = \sum_{k=2}^{n} \sum_{l=0}^{k-2} \sum_{j=0}^{l} A_{k}\left( S^{l}N_{0}S^{j}NS^{n-l-j-2} + S^{l}NS^{j}N_{0}S^{n-l-j-2} \right) \\
	& = 2ab \sum_{k=2}^{n} \sum_{l=0}^{k-2} \sum_{j=0}^{l} A_{k}\left( S^{l} \vec^{-1}(\chi_{1}) S^{j} \vec^{-1}(\chi_{1}) S^{n-l-j-2} \right) \neq 0
	\end{align*}
	where $N = a\vec^{-1}(\chi_{1})$ and $N_{0} = b\vec^{-1}(\chi_{1})$.
	Moreover, $P''_{S}(N_{0},N)$ is either positive or negative.
	
	On the other hand,
	$$
	\vec\left( P''_{S}(N_{0},N) \right) = k_{1}\chi_{1}+k_{2}\chi_{2}+\cdots+k_{n^{2}}\chi_{n^{2}}
	$$
	for some real numbers $k_{1},k_{2},\cdots,k_{n^{2}}$.
	By Fundamental theorem of linear algebra in \cite{strang2006linear} and Lemma 6.3.10 in \cite{Horn1985}, we have
	\begin{equation}\label{eq:yTPtYx}
	\psi^{T} \vec\left( P''_{S}(N_{0},N) \right) = k_{1}\psi^{T}\chi_{1}.
	\end{equation}
	Since $P''_{S}(N_{0},N)$ is either positive or negative and $\psi$ is positive, the left side of \eqref{eq:yTPtYx} is also either positive or negative.
	So, $k_{1}$ cannot be zero.
	Therefore,
	$$
	\pjn \left( P''_{S}(N_{0},N) \right) = k_{1}\vec^{-1}(\chi_{1}) \neq 0.
	$$
	
\end{proof}





\begin{lemma}\label{lem:TaylorLemma}
	Let $S$ be the minimal positive solution of \eqref{eq:MPE} with Assumption \ref{ass:MPE},
	and let $\{X_{i}\}_{i=0}^{\infty}$ be a Newton sequence in \eqref{eq:NewtonStepMPE2}. Then,
	$$
	\| P({X}_{i})\|\leq a\|\tXi\|^{2}+b\|\tXi\|\|\tilde{X}_{i-1}\|+c\|\tilde{X}_{i-1}\|^{2}
	$$
	for some positive real numbers $a,b,c$.
\end{lemma}

\begin{proof}
	From Taylor's Theorem and putting $S = X_{i-1} - \tilde{X}_{i-1}$, we have
	\begin{equation}\label{eq:Taylor}
	P(X_{i}) = P(S) + P'_{S}(X_{i} - S) + O(\|X_{i} - S\|^{2}),
	\end{equation}
	and
	\begin{equation*}
	0 = P(S) = P(X_{i-1} - \tilde{X}_{i-1}) = P(X_{i-1}) - P'_{X_{i-1}}(\tilde{X}_{i-1}) + O(\|\tilde{X}_{i-1}\|^{2}),
	\end{equation*}
	which is equivalent to
	\begin{equation}\label{eq:TaylorMPE3}
	- P(X_{i-1}) + P'_{X_{i-1}}(\tilde{X}_{i-1}) = O(\|\tilde{X}_{i-1}\|^{2}).
	\end{equation}
	From \eqref{eq:NewtonStepMPE}, we have
	\begin{equation}\label{eq:NewtonStepAnother}
	0 = P'_{X_{i-1}}(X_{i} - X_{i-1}) + P(X_{i-1}),
	\end{equation}
	and clearly
	\begin{equation}
	X_{i} - X_{i-1} = \tXi - \tilde{X}_{i-1}.
	\end{equation}
	If we subtract \eqref{eq:NewtonStepAnother} from \eqref{eq:Taylor} 
	and substitute \eqref{eq:TaylorMPE3}, we obtain
	\begin{align*}
	P(X_{i}) & = P(S) + P'_{S}(\tXi) - P(X_{i-1}) - P'_{X_{i-1}}(\tXi - \tilde{X}_{i-1}) + O(\|\tXi\|^{2}) \\
	& = P'_{S}(\tXi) - P'_{X_{i-1}}(\tXi) + O(\| \tXi \|^{2}) + O(\|\tilde{X}_{i-1}\|^{2}).
	\end{align*}
	Putting $S = X_{i-1} - \tilde{X}_{i-1}$ in the previous equality,
	\begin{align*}
	P(X_{i}) & = \sum_{k=1}^{n}\sum_{l=0}^{k-1}A_{k}(X_{i-1} - \tilde{X}_{i-1})^{l}\tXi(X_{i-1} - \tilde{X}_{i-1})^{k-l-1} \\
	& \qquad\qquad\qquad\qquad - P'_{X_{i-1}}(\tXi)	+ O(\| \tXi \|^{2}) + O(\|\tilde{X}_{i-1}\|^{2})\\
	& = \sum_{k=1}^{n}\sum_{l=0}^{k-1}A_{k}X_{i-1}^{l}\tXi X_{i-1}^{k-l-1} + O(\|\tXi\|\|\tilde{X}_{i-1}\|)\\
	& \qquad\qquad\qquad\qquad - P'_{X_{i-1}}(\tXi)	+ O(\| \tXi \|^{2}) + O(\|\tilde{X}_{i-1}\|^{2})\\
	& = P'_{X_{i-1}}(\tXi)+O(\|\tXi\|\|\tilde{X}_{i-1}\|)- P'_{X_{i-1}}(\tXi)+O(\| \tXi \|^{2}) + O(\|\tilde{X}_{i-1}\|^{2})\\
	& = O(\| \tilde{X}_{i-1} \|^{2}) + O(\| \tXi \|\| \tilde{X}_{i-1} \|) + O(\| \tXi \|^{2}).
	\end{align*}
	Since $\|\cdot\|$ is a multiplicative matrix norm on $\mathbb{R}^{m\times m}$,
	we have required result.
\end{proof}




\begin{lemma}\label{lem:DominatedPMconvergence}
	For any fixed $\theta > 0$, let
	$$
	\mathcal{Q} = \{i \in \NN \cup \{0\}|\|\pjm(\tXi)\| > \theta \|\pjn(\tXi)\|\}
	$$
	where $\{X_{i}\}$ is the Newton sequence in Theorem \ref{thm:ConvergenceNewtonMPE}.
	Then, there exist an integer $i_{0}$ and a constant $c > 0$ such that
	$\| \tXi\| \leq c \| \tilde{X}_{i-1} \|^{2}$ for all $i\geq i_{0}$ in $\mathcal{Q}$.
\end{lemma}

\begin{proof}
	Using Taylor's Theorem and the fact that $P'_{S}\left(\pjn(\tXi)\right) = 0$,
	\begin{equation}\label{eq:TaylorMPE}
	P(X_{i}) = P(S) + P'_{S}(\tXi) + O(\|\tXi\|^{2}) = P'_{S}\left(\pjm(\tXi)\right) + O(\|\tXi\|^{2}).
	\end{equation}
	Since $P'_{S}|_{\M} : \M \rightarrow \M$ is
	invertible, $\left\| P'_{S}\left(\pjm(\tXi)\right)\right\| \geq c_{1}\|\pjm(\tXi)\|$
	for some constant $c_{1}>0$.
	For $i\in\mathcal{Q}$, we have
	\begin{equation*}
	\|\tXi\| \leq \left\| \pjm(\tXi)\right\| + \left\| \pjn(\tXi)\right\|
	\leq (\theta^{-1} + 1) \left\| \pjm(\tXi)\right\|.
	\end{equation*}
	Thus by \eqref{eq:TaylorMPE},
	\begin{equation}\label{eq:NormofhatXi}
	\|P(X_{i})\| \geq c_{1}\|\pjm(\tXi)\| - c_{2}\|\tXi\|^{2} \geq c_{1}(\theta^{-1}+1)^{-1}\|\tXi\| - c_{2}\|\tXi\|^{2}.
	\end{equation}
	On the other hand, from Lemma \ref{lem:TaylorLemma}, we have
	$$
	\| P({X}_{i})\|\leq c_{3}\|\tXi\|^{2}+c_{4}\|\tilde{X}_{i-1}\|\|\tXi\|+c_{5}\|\tilde{X}_{i-1}\|^{2}.
	$$
	From \eqref{eq:NormofhatXi} and the fact that $X_{i}\neq S$ for
	any $i$, we have
	$$
	c_{1}(\theta^{-1}+1)^{-1} - c_{2}\|\tXi\| \leq c_{3}\|\tXi\| + c_{4}\|\tilde{X}_{i-1}\| + c_{5}\frac{\|\tilde{X}_{i-1}\|^{2}}{\|\tXi\|}.
	$$
	Since $\tXi$ converges to $0$ by Theorem \ref{thm:ConvergenceNewtonMPE},
	we can find an $i_{0}$ such that $\|\tXi\|\leq c\|\tilde{X}_{i-1}\|^{2}$
	for all $i\geq i_{0}$.\end{proof}
\begin{corollary}
	Assume that, for given $\theta>0$, $\|\pjm(\tXi)\|>\theta\|\pjn(\tXi)\|$
	for all $i$ large enough. Then $X_{i}\rightarrow S$ quadratically.
\end{corollary}


When $P'_{S}$ is singular practically the Newton sequence
converges linearly, according to the corollary we conclude that \textit{the
	error will generally be dominated by its $\N$ component}\cite{Guo1998M}.
From Lemmas \ref{lem:BisInvertible} and \ref{lem:DominatedPMconvergence}
we have the following main theorem.

\begin{theorem}\label{thm:LinearConvergence1over2}
	If $-\P'_{S}$ is a singular $M$-matrix and the convergence rate
	of the Newton sequence $\{X_{i}\}$ in Theorem \ref{thm:ConvergenceNewtonMPE}
	is not quadratic, then $\| P_{X_{i}}^{\prime -1}\|\leq c\| \tXi\|^{-1}$
	for all $i \geq 1$ and some constant $c > 0$. Moreover,
	$$
	\lim_{i \rightarrow \infty} \frac{\|\tXii\|}{\|\tXi\|} = \frac{1}{2},
	\qquad \lim_{i\rightarrow\infty} \frac{\|\pjm(\tXi)\|}{\|\pjn(\tXi)\|^{2}}=0.
	$$
	
\end{theorem}

\section{Note}

\begin{align*}
	-A_{1} &= rI_{m} - N \\
	-A_{1}^{-1} &= \frac{1}{r}\sum_{n=0}^{\infty} \left( \frac{N}{r} \right)^{n} \\
	-A_{1}^{-1}A_{0} &= \frac1r \sum_{n=0}^{\infty}\left( \frac{N}{r} \right)^{n}A_{0} = X_{1} > 0 \\
	-A_{1}-\e I_{m} &= (r - \e)I_{m} - N \\
	(-A_{1} - \e I_{m})^{-1} &= \frac{1}{r - \e} \sum_{n=0}^{\infty} \left( \frac{N}{r - \e} \right)^{n} \\
	&= \sum_{n=0}^{\infty} (-\e A_{1}^{-1})^{n}(-A_{1}^{-1}) \\
	&= \sum_{n=0}^{\infty} \e^{n} (-A_{1}^{-1})^{n+1} \\
	&= -A_{1}^{-1} + \sum_{n=1}^{\infty} \e^{n} (-A_{1}^{-1})^{n+1} \\
	0 &= X_{0} < X_{1} \leq \ldots \leq X_{i} \leq \ldots \leq S \\
	-X_{0} &> -X_{1} \geq \ldots \geq -X_{i} \geq \ldots \geq -S \\
	(-A_{1} - \e I_{m})^{-1}(A_{0} - \e S) &= \sum_{n=0}^{\infty} \e^{n} (-A_{1}^{-1})^{n+1} (A_{0} - \e S) \\
	&= -A_{1}^{-1} (A_{0} - \e S) + \sum_{n=1}^{\infty} \e^{n} (-A_{1}^{-1})^{n+1} (A_{0} - \e S) \\
	(-A_{1} - \e I_{m})^{-1}(A_{0} - \e S) &= \frac{1}{r - \e}\sum_{n=0}^{\infty}\left(\frac{N}{r - \e}\right)^{n}A_{0} - \frac{\e}{r - \e}\sum_{n=0}^{\infty}\left(\frac{N}{r - \e}\right)^{n}S
\end{align*}

a
%$$
%Q_{\e}(X) = AX^{2} + (B - \e I)X + (C + \e S) = 0
%$$
%
%\begin{equation}
%P_{\e}(X) = A_{0} + \e S + (A_{1} - \e I)X + \sum_{k=2}^{n}A_{k}X^{k},
%\end{equation}
%
%\begin{align*}
%\P'_{\e}[X] &= I \otimes (A_{1} - \e I) + \sum_{k=2}^{n}\sum_{l=0}^{k-1}(X^{k-l-1})^{T} \otimes A_{k}X^{l}\\
%&= \P'_{0}[X] - \e (I \otimes I),
%\end{align*}
%
%\begin{center}
%	=============================
%\end{center}
%
%\begin{equation}
%	P(X) = \sum_{k=0}^{n} A_{k}X^{k}
%\end{equation}
%
%\begin{equation}
%	-\P'[S]{\bf e} = \e {\bf e}
%\end{equation}
%
%\begin{equation}
%	R(X) = \sum_{k=2}^{n} A_{k}X^{k} + (A_{1} + \e I)X + A_{0}
%\end{equation}
%
%\begin{align*}
%	\mathcal{R}'[X] &= I \otimes (A_{1} + \e I) + \sum_{k=2}^{n}\sum_{l=0}^{k-1}(X^{k-l-1})^{T} \otimes A_{k}X^{l}\\
%					&= \P'[X] + \e (I \otimes I)
%\end{align*}
%
%\begin{align*}
%	-\mathcal{R}'[X]{\bf e} &= (-\P'[X] - \e (I \otimes I)){\bf e}\\
%							&= -\P'[X]{\bf e} - \e (I \otimes I){\bf e}\\
%							&= \e{\bf e} - \e{\bf e}\\
%							&= 0
%\end{align*}



%Now, we consider an MPE with other coefficients given by
%\begin{equation}\label{eq:eMPE}
%P_{\e}(X) = (A_{0} + \e S) + (A_{1} - \e I)X + \sum_{k=2}^{n}A_{k}X^{k},
%\end{equation}
%where $\e \geq 0$ and $S$ is the minimal positive solution in Theorem \ref{thm:ConvergenceNewtonMPE}.
%Then, \eqref{eq:eMPE} also satisfies Assumption \ref{ass:MPE}.
%Moreover, for $Y_{1} \leq Y_{2} \in L[0,S]:=\{Y \in \RR^{\mbym} ~|~ 0 \leq Y \leq S\}$,
%\begin{align*}
%	(P'_{\e}[Y_{2}] - P'_{\e}[Y_{1}])(Y_{2} - Y_{1}) &= \sum_{k=2}^{n} \sum_{l=0}^{k-1} A_{k}(Y_{2}^{l}(Y_{2}-Y_{1})Y_{2}^{k-l-1} - Y_{1}^{l}(Y_{2}-Y_{1})Y_{1}^{k-l-1})\\
%	&\qquad + (A_{1}-\e I)(Y_{2}-Y_{1}) - (A_{1}-\e I)(Y_{2}-Y_{1})\geq 0,
%\end{align*}
%i.e., $P_{\e}$ is order-convex on $L[0,S]$.
%Since $P_{\e}(0) = A_{0} + \e S \geq 0$ and $P_{\e}(S) = 0$, $S$ is the unique solution for $P_{\e}(X) = 0$ on $L[0,S]$ by \cite[Monotone Newton Theorem]{Ortega2000}.
%Thus, $P_{\e}(X) = 0$ has $S$ as the minimal nonnegative solution although the coefficients of $P_{\e}$ do not satisfy \eqref{eq:SuffMPE}, always.


\bibliographystyle{plain}
\bibliography{SHSeo}

\end{document}