%% LyX 1.6.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[10pt,a4paper,english]{amsart}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage{endnotes}
\usepackage{units}
%\usepackage{multirow}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{cite}
%\usepackage{natbib}
\usepackage{amsthm}
\usepackage{array,arydshln}
\usepackage[pdftex]{graphicx}
\usepackage{rotating}
\usepackage{ifpdf}
%\usepackage{epsfig}
\usepackage[all]{xy}
\usepackage{latexsym}
\usepackage[hidelinks]{hyperref}
\usepackage{color}
\usepackage{eucal}
\usepackage{mathrsfs}
\usepackage{kotex}
\usepackage{slashbox}
%\usepackage{ulem}
%\usepackage{hfont}


\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section} %% Comment out for sequentially-numbered
\numberwithin{figure}{section} %% Comment out for sequentially-numbered
\numberwithin{table}{section}
\let\footnote=\endnote
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{theorem}[thm]{Theorem}
\newtheorem{Theorem}[thm]{Theorem}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{Def}[thm]{Definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{exam}[thm]{Example}
\newtheorem{algo}[thm]{Algorithm}
%  \theoremstyle{plain}
\newtheorem{assumption}[thm]{Assumption}
\theoremstyle{plain}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{lemma}[thm]{Lemma}
\theoremstyle{plain}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{corollary}[thm]{Corollary}
\theoremstyle{plain}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{rem}[thm]{Remark}

\def\norm#1{\|#1\|}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%\newcommand{\norm}[1]{\|#1\|}
\def\norm#1{\|#1\|}
\def\normm#1#2{\|#1\|_{#2}}
\def\normF#1{\|#1\|_{F}}
\def\Proof{{\bf Proof.\enspace}}
\def\vec{{\sf vec}}
%\def\unvec{\mathrm{unvec}}
\def\tr{\mathrm{tr}}
%\def\tr{\textrm{tr}}
\def\bmatrix#1{\left[\begin{matrix}#1\end{matrix}\right]}
\def\pmatrix#1{\left(\begin{matrix}#1\end{matrix}\right)}
\def\RR{\mathbb{R}}
\def\NN{\mathbb{N}}
\def\CC{\mathbb{C}}
\def\nbyn{n\times n}
\def\mbyn{m\times n}
\def\mbym{m\times m}
\def\pbyq{p\times q}
\def\nnbynn{n^{2}\times n^{2}}
\def\mbf#1{\mathbf{#1}}
\def\mrm#1{\mathrm{#1}}
\def\bpi{\boldsymbol{\pi}}
\def\a{\alpha}
\def\b{\beta}
\def\d{\delta}
\def\e{\varepsilon}
\def\l{\lambda}
\def\D{\mathfrak{D}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\Q{\mathcal{Q}}
\def\M{\mathcal{M}}
\def\N{\mathcal{N}}
\def\P{\mathcal{P}}
\def\X{\mathfrak{X}}
\def\proj{\mathbf{P}}
\def\pjn{\mathbf{P}_{\mathcal{N}}}
\def\pjm{\mathbf{P}_{\mathcal{M}}}
\def\tXi{\tilde{X}_{i}}
\def\tXii{\tilde{X}_{i+1}}
\def\dpm#1{\begin{displaymath}#1\end{displaymath}}
\def\bdm{\begin{displaymath}}
\def\edm{\end{displaymath}}
\def\dtyl{\displaystyle}
\def\ones#1{\mathbf{1}_{#1}}
%\def\onesn{\mathbf{1}_{n \times n}}

\definecolor{gray}{rgb}{.7,.7,.7}

\makeatother

\begin{document}
	
\title[Solving Nearly Non-simple MPE by Newton's Method with Line Searches]{Solving Nearly Non-simple Matrix Polynomial Equations by Newton's Method with Advanced Line Searches}
\author{Sang-hyup Seo}
\address{Sang-hyup Seo\\Where}
\email{saibie1677@gmail.com}
\date{\today}

\begin{abstract}
%	We consider the Newton iteration for a matrix polynomial equation which arises in stochastic problem.
%	In this paper, we show that the elementwise minimal nonnegative solution of the matrix polynomial equation can be obtained using Newton's method if the equation satisfies the sufficient condition, and
%	the convergence rate of the iteration is quadratic if the solution is simple.
%	Moreover, we show that the convergence rate is at least linear if the solution is non-simple,
%	but we can apply a modified Newton method whose iteration number is less than the pure Newton iteration number.
%	Finally, we give a numerical experiment which is related with our issue.
\end{abstract}

\keywords{matrix polynomial equation, positive solution, nonnegative solution, $M$-matrix, Newton's method, line search, acceleration of a method, nearly non-simple}

\subjclass[2010]{65H10}

% \thanks{$\dagger$Corresponding Author}

\maketitle

\section{Introduction}\label{sec:intro}

We consider a matrix polynomial equation(MPE) with $n$-degree defined by
\begin{equation}\label{eq:MPE}
P(X)=\sum_{k=0}^{n} A_{k}X^{k}= A_{0}+A_{1}X+\cdots+A_{n-1}X^{n-1} + A_{n}X^{n}=0,
\end{equation}
where the coefficient matrices $A_{k}$'s are $\mbym$ matrices.
Then, the unknown matrix $X$ must be an $\mbym$ matrix.
A matrix $S$ is called a solution of \eqref{eq:MPE} if $P(S) = 0$.


The MPE \eqref{eq:MPE} often occurs in the theory of differential equations, system theory, network theory, stochastic theory, quasi-birth-and-death
and other areas \cite{ALFA2003, Bean1997, Butler1985, Gohberg1982, Lancaster1966, Lancaster1985, He2001, bini2005, Latouche1999}.
Specially, in quasi-birth-and-death and stochastic problems, finding the minimal nonnegative solution of a matrix equation is an important issue.

There are many researches to find the minimal nonnegative solution.
%Davis \cite{Davis1981, Davis1983} and Higham, Kim \cite{Higham2000, Higham2001} studied the Newton method for a quadratic matrix equation.
Guo and Laub \cite{Guo2000} considered a nonsymmetric algebraic Riccati equation, and they proposed iteration algorithms which converge to the minimal positive solution. 
In \cite{Guo2001}, Guo provided a sufficient condition for the existence of nonnegative solutions of nonsymmetric algebraic Riccati equations.
Kim \cite{Kim2008} showed that the minimal positive solutions also can be found by the Newton method with the zero initial matrices in some different types of quadratic equations.
%Hautphenne, Latouche, and Remiche \cite{SophieHautphenne2008} studied the Newton method for the Markovian binary tree.
Seo and Kim \cite{SeoSeoKim2013, SeoKim2014} studied the Newton iteration for matrix polynomial equations.

Newton's method is one of powerful tools to find solutions of nonlinear matrix equations.
By Kantorovich theorem \cite{Kantorovich1964}, the convergence rate of the method is quadratic if the derivative on the domain is Lipschitz continuous and at the solution is nonsingular.
But, if the derivative at the solution is singular, then we cannot apply Kantrovich theorem, i.e., we cannot guarantee that the rate is quadratic.
The followings are researches to analyze the problems with singular derivative at the solution and improve the method.

For general functions on Banach spaces, Reddien \cite{Reddien1978}, Decker and Kelley \cite{DeckerKelley1980Newton1, DeckerKelley1980Newton2} gave analyses about Newton's method for singular problems.
In \cite{DeckerKellerKelley1983}, Decker, Keller, and Kelley provided an acceleration of Newton's method for singular problems and analyzed for the convergence rate of the method.
Kelley and Suresh \cite{Kelley1983} suggested a new accelerated Newton's method at singular points.
Decker and Kelley \cite{DeckerKelley1985} showed an analysis for Newton's method
at nearly singular roots.
In \cite{Kelley1986}, Kelley analyzed convergence rate of the method for functions whose high order derivatives at the solution are singular.
For specific functions on $\RR^{\mbyn}$, Guo and Lancaster \cite{Guo1998M} analyzed and provided a modification about Newton's method for algebraic Riccati equations at singular roots.
In \cite{SeoSeoKim2018}, S-.H. Seo, J-.H. Seo, and H-.M. Kim suggested a modified Newton method for matrix polynomial equations with $n$-degree for singular problems.

In \cite{Kelley1983, DeckerKellerKelley1983}, accelerations for Newton's method at singular roots was suggested for general functions on Banach spaces.
For specific functions on $\RR^{\mbyn}$, modifications of Newton's method at singular roots was provided in \cite{Guo1998M, SeoSeoKim2018}.
But, the main hypotheses of the papers are similar and that the solution is non-simple, i.e., the derivative at the solution is singular.
It means that the accelerations for Newton's method cannot be applied and the accelerated iterations cannot be guaranteed converge to the solutions if the solution is simple.
Even if the iterations converge to the solutions, they cannot be guaranteed better than the pure iterations.


Otherwise, in \cite{DeckerKelley1985}, authors analyzed and suggested an acceleration for Newton's method about the case of nearly singular roots with the general bifurcation problem \cite{Keller1977, Stakgold1971}.
In \cite{SeoKim2014}, it was suggested that a global accelerated Newton's method, which can be applied whether the solution is simple or not, for \eqref{eq:MPE}.

The aim of this paper is also to provide an improved Newton's method for \eqref{eq:MPE} which satisfies following Assumption \ref{ass:MPE} at nearly singular roots.
We show that the differences between the Newton sequences and the solution lie on nearly one-dimensional space.
Moreover, with the exact line search idea in \cite{Seo2008}, we suggest a proper acceleration weight $\a$.

\begin{assumption}\label{ass:MPE}
	For the MPE \eqref{eq:MPE},
	\begin{enumerate}[1)]
		\item The coefficient matrices $A_{k}$'s are nonnegative except $A_{1}$.
		\item $-A_{1}$ is a nonsingular $M$-matrix.
		\item $A_{0}$, $A_{1}$, and $\sum_{k=2}^{n}A_{k}$ are irreducible.
	\end{enumerate}
\end{assumption}


Here, we give some basic definitions and lemmas for this paper.

Let $A, B \in \RR^{\mbym}$ be matrices.
If all elements of $A$ is nonnegative, then we call that $A$ is a {\it nonnegative} matrix and denote $A \geq 0$. In similar sense, we define $A \leq 0$, $A > 0$, and $A < 0$.
If a matrix $A$ can be written as $rI_{m}-B$ with $B \geq 0$ and $r \in \RR$, we call that $A$ is a \textit{$Z$-matrix}.
Moreover, if $A=rI_{m}-B$ is a $Z$-matrix and $r\geq\rho(B)$ then $A$ is called an $M$-matrix.
The following is a basic theorem for $M$-matrices.

\begin{thm}\label{thm:proMmat}%{\rm \cite[Theorem 2.1]{Guo2007}, \cite[Theorem 2.1]{Poole1974}}
	For a $Z$-matrix $A$, the following are equivalent:
	\begin{enumerate}
		\item $A$ is a nonsingular $M$-matrix.
		
		\item $A^{-1}$ is nonnegative.
		
		\item $Av>0$ for some vector $v>0$.
		
		\item All eigenvalues of $A$ have positive real parts.
	\end{enumerate}
\end{thm}

Let a function $F : \RR^{\mbyn} \rightarrow \RR^{\mbyn}$ and an equation $F(X) = 0$ be given.
If nonnegative solutions $S_{1}$ and $S_{2}$ of $F(X) = 0$ are satisfy
	\begin{equation}\label{eq:s1ss2}
	S_{1}\leq S\leq S_{2},\end{equation}
for any nonnegative solution $S$ of $F(X) = 0$, then they are called the {\it minimal nonnegative solution} and the {\it maximal nonnegative solution}, respectively.
The {\it minimal positive solution} and the {\it maximal positive solution} also are can be defined, similarly.
If the Fr\'echet derivative of $F$ at a solution $S$ is nonsingular, then $S$ is called {\it simple}.
Furthermore, we call that a not simple solution is {\it non-simple}.
In this paper, for convenience, the notation $||\cdot||$ is used instead of the Frobenius norm $||\cdot||_{F}$ and $\tilde{X}$ denotes $S - X$, the difference between $X$ and the solution $S$, 
because they are used very frequently.




\section{Convergence of Newton's Method for the MPE}\label{sec:analysis}

In this paper, we denote that
\begin{equation}\label{eq:DerivativeMPE}
P'[X](H)=\sum_{k=1}^{n} \sum_{l=0}^{k-1} A_{k}X^{l}HX^{k-l-1}
\end{equation}
is the Fr\'{e}chet derivative of the MPE \eqref{eq:MPE} at $X$ in the direction $H$, and
\begin{equation}\label{eq:2DerivativeMPE}
P''[X](K,H)=\sum_{k=2}^{n}\sum_{l=0}^{k-2}\sum_{j=0}^{l} A_{k} \left( X^{l}HX^{j}KX^{n-l-j-2} + X^{l}KX^{j}HX^{n-l-j-2} \right)
\end{equation}
is the second Fr\'{e}chet derivative at $X$.

For given initial guess $X_{0}$, the Newton iteration of \eqref{eq:MPE},
\begin{equation}\label{eq:NewtonStepMPE2}
X_{i+1}=X_{i}-P'[X_{i}]^{-1}(P(X_{i})),\quad i=0,1,2,\ldots
\end{equation}
can be separated into two parts as
\begin{equation}\label{eq:NewtonStepMPE}
\begin{cases}
P'[X_{i}](H_{i}) = -P(X_{i}),\\
X_{i+1}=X_{i}+H_{i},\end{cases}\quad i=0,1,2,\ldots.
\end{equation}
If we define that
\begin{equation}
	\P'[X] = \vec \circ P[X]^{\prime} \circ \vec^{-1} = \sum_{k=1}^{n} \sum_{l=0}^{k-1} (X^{k-l-1})^{T} \otimes A_{k}X^{l},
\end{equation}
then solving the above equation of \eqref{eq:NewtonStepMPE} is equivalent to solving the following $m^{2}\times m^{2}$ linear system
\begin{equation}
\P'[X_{i}]\vec(H_{i}) = \vec(-P(X_{i})),
\end{equation}
where $\vec$ is the vectorization by \cite[Lemma 4.3.1]{Horn1994}.



We introduce a sufficient condition of the existence of
the minimal nonnegative solution of the MPE \eqref{eq:MPE} with Assumption \ref{ass:MPE},
and give some analysis for Newton's method.

\begin{theorem}\label{thm:SuffMPE}{\rm \cite[Theorem 2.1]{Meng2017}}
	Let the MPE \eqref{eq:MPE} with 1) and 2) in Assumption \ref{ass:MPE} be given.
	Then, there exists the minimal nonnegative solution if
	\begin{equation}\label{eq:SuffMPE}
	-\sum_{k=0}^{n}A_{k}~\textrm{is a nonsingular or singular irreducible $M$-matrix.}
	\end{equation}
	
\end{theorem}




\begin{theorem}\label{thm:ConvergenceNewtonMPE}{\rm\cite[Theorem 2.2]{SeoSeoKim2018}}
	Suppose that the MPE \eqref{eq:MPE} satisfies Assumption \ref{ass:MPE} and \eqref{eq:SuffMPE}.
	Then, the Newton sequence $\{X_{i}\}$ with $X_{0} = 0$ is well defined, is monotone nondecreasing,
	and converges to the minimal positive solution $S$.
	Furthermore, $-\P'[X_{i}]$ is a nonsingular irreducible $M$-matrix for $i \in \NN$, and $-\P'[S]$ is an irreducible $M$-matrix.
\end{theorem}





\section{The Nearly Singular Derivative $-P'[S]$}\label{sec:nearlysingular}
Generally, by Kantorovich theorem \cite{Kantorovich1964}, the Newton iteration \eqref{eq:NewtonStepMPE2} converges quadratically if the derivative at the solution is nonsingular.
But, if the derivative becomes nearly singular, the quadratic convergence radius will be very small.
In this section, we will see the converging tendency of \eqref{eq:NewtonStepMPE2} when the derivative is nearly singular.

Let the MPE \eqref{eq:MPE} satisfy Assumption \ref{ass:MPE}, $S$ be the minimal positive solution of \eqref{eq:MPE}.
Then, $-\P'[S]$ is an irreducible $M$-matrix.
Therefore, the eigenvalue $\mu$ of $-\P'[S]$ whose absolute value is the smallest in absolute values of all eigenvalues is nonnegative and simple by Perron-Frobenius Theorem in \cite{Horn1985}.

Here, we think over an other MPE,
\begin{equation}\label{eq:epsMPE}
	P_{\e}(X) = (A_{0} - (\mu - \e) S) + (A_{1} + (\mu - \e) I_{m})X + \sum_{k=2}^{n}A_{k}X^{k} = 0,
\end{equation}
for $\e \geq 0$.
Clearly, $S$ is a solution of \eqref{eq:epsMPE}.
Since the representation of the Fr\'echet derivative of $P_{\e}$ at $S$ is that
\begin{align*}
\P'_{\e}[S] &= I_{m} \otimes (A_{1} + (\mu - \e) I_{m}) + \sum_{k=2}^{n}\sum_{l=0}^{k-1}(S^{k-l-1})^{T} \otimes A_{k}S^{l}\\
&= \P'[S] + (\mu - \e) I_{m^2},
\end{align*}
$-\P'_{\e}[S]$ is an $M$-matrix by Theorem \ref{thm:proMmat}, and the nonnegative number $\e$ is the smallest simple eigenvalue of $-\P'_{\e}[S]$.
%In addition, $-A_{1} - \e I_{m}$ is also an $M$-matrix.
Here, we assume that
%\begin{equation}
%		-P'_{\e}[S](N) = \e N, \quad \N = {\rm span}(N),
%\end{equation}
%where 
\begin{equation}
	N = \vec^{-1}({\bf n}), \quad \N = {\rm span}(N), \quad \RR^{\mbym} = \N \oplus \M,
\end{equation}
where ${\bf n}$ is a unit eigenvector of $-\P'[S]$ corresponding to $\mu$.
In addition, we define $\pjn$ is a projection onto $\N$ parallel to $\M$, and $\pjm = I - \pjn$.

\begin{lemma}\label{lem:BisInvertible}
	Suppose the MPE \eqref{eq:MPE} satisfies Assumption \ref{ass:MPE} and \eqref{eq:epsMPE} is given.
	Then, $\e$ is a simple eigenvalue of $-\mathcal{P}'_{\e}[S]$, $\pjn P''_{\e}[S](N, N) \neq 0$, and $P'_{\e}[S](\M) = \M$.
\end{lemma}

\begin{proof}
	Since $S$ is positive and $A_{k}$'s are irreducible, $-\P'[S]$ is irreducible.
	Thus, $-\P'_{\e}[S] = -\P'[S] + (\e - \mu)I_{m}$ is also irreducible as the diagonal entries do not effect on irreducibility of matrices.
	Then, by Perron-Frobenius Theorem \cite{Horn1985}, $\e$ is a simple eigenvalue of $-\P'_{\e}[S]$ with a positive eigenvector. Thus, we can find $m^{2}$ linearly independent
	vectors $\chi_{1},\chi_{2},\cdots \chi_{m^{2}}$ such that $\chi_{1}>0$ and
	
	\begin{equation}\label{eq:invUDsU}
	\X^{-1}(-\P'_{\e}[S])\X=\bmatrix{
		\e & 0\\
		0 & \D}, ~\textrm{where }\X=\left[\begin{array}{c|c|c|c} \phantom{1}&\phantom{1}&\phantom{1}&\phantom{1}\\ \chi_{1}&\chi_{2}&\cdots&\chi_{m^{2}}\\\phantom{1}&\phantom{1}&\phantom{1}&\phantom{1}
	\end{array} \right]
	\end{equation}
	and $\D$ is an $(m^{2}-1)\times(m^{2}-1)$ nonsingular matrix, i.e., $\M = {\rm span}(\vec^{-1}(\chi_{2}), \ldots, \vec^{-1}(\chi_{m^{2}}))$ and $P'_{\e}[S](\M) = \M$.
	By the same way, we also have a positive vector $\psi_{1}$ such that $-\psi_{1}^{T}\P'_{\e}[S] = \e \psi_{1}^{T}$.
	%	Now, $P'_{S}(N) = 0$ if and only if $\P'_{S}\mathrm{vec}(N)=0$.
	Since $N \in \N = {\rm span}(\vec^{-1}(\chi_{1}))$, there exists nonzero $a \in \RR$ such that $N = a\vec^{-1}(\chi_{1})$.
	From $\vec^{-1}(\chi_{1}) > 0$ and $S > 0$, we have
	\begin{align*}
		P''_{\e}[S](N,N) &= P''[S](N,N) \\
		& = \sum_{k=2}^{n} \sum_{l=0}^{k-2} \sum_{j=0}^{l} A_{k}\left( S^{l}NS^{j}NS^{n-l-j-2} + S^{l}NS^{j}NS^{n-l-j-2} \right) \numberthis \\
		& = 2a^{2} \sum_{k=2}^{n} \sum_{l=0}^{k-2} \sum_{j=0}^{l} A_{k}\left( S^{l} \vec^{-1}(\chi_{1}) S^{j} \vec^{-1}(\chi_{1}) S^{n-l-j-2} \right) > 0.
	\end{align*}
	On the other hand,
	\begin{equation}
		\vec\left( P''[S](N,N) \right) = k_{1}\chi_{1}+k_{2}\chi_{2}+\cdots+k_{m^{2}}\chi_{m^{2}}
	\end{equation}
	for some real numbers $k_{1},k_{2},\ldots,k_{m^{2}}$.
	By Fundamental theorem of linear algebra in \cite{strang2006linear} and Lemma 6.3.10 in \cite{Horn1985}, we have
	\begin{equation}\label{eq:k1psiTchi1}
	\psi_{1}^{T} \vec\left( P''[S](N,N) \right) = k_{1}\psi_{1}^{T}\chi_{1}.
	\end{equation}
	Since all of $P''[S](N,N)$, $\chi_{1}$, and $\psi_{1}$ are positive, the left side of \eqref{eq:k1psiTchi1} is positive, i.e., $k_{1}$ must be positive.
	Therefore,
	\begin{equation}
		\pjn \left( P''[S](N,N) \right) = k_{1}\vec^{-1}(\chi_{1}) > 0.
	\end{equation}	
\end{proof}


\begin{lemma}{\rm \cite[Lemma 3.3]{SeoSeoKim2018}}\label{lem:TaylorLemma}
	Let $S$ be the minimal positive solution of \eqref{eq:MPE} with Assumption \ref{ass:MPE},
	and let a Newton sequence $\{X_{\e,i}\}_{i=0}^{\infty}$ for \eqref{eq:epsMPE} converge to $S$. Then,
	\begin{equation}
		\| P_{\e}({X}_{\e, i})\|\leq a\|\tilde{X}_{\e,i}\|^{2}+b\|\tilde{X}_{\e,i}\|\|\tilde{X}_{\e,i-1}\|+c\|\tilde{X}_{\e, i-1}\|^{2}
	\end{equation}
	for some positive real numbers $a,b,c$.
\end{lemma}

%\begin{proof}
%	From Taylor's Theorem and putting $S = X_{i-1} - \tilde{X}_{i-1}$, we have
%	\begin{equation}\label{eq:Taylor}
%	P(X_{i}) = P(S) + P'_{S}(X_{i} - S) + O(\|X_{i} - S\|^{2}),
%	\end{equation}
%	and
%	\begin{equation*}
%	0 = P(S) = P(X_{i-1} - \tilde{X}_{i-1}) = P(X_{i-1}) - P'_{X_{i-1}}(\tilde{X}_{i-1}) + O(\|\tilde{X}_{i-1}\|^{2}),
%	\end{equation*}
%	which is equivalent to
%	\begin{equation}\label{eq:TaylorMPE3}
%	- P(X_{i-1}) + P'_{X_{i-1}}(\tilde{X}_{i-1}) = O(\|\tilde{X}_{i-1}\|^{2}).
%	\end{equation}
%	From \eqref{eq:NewtonStepMPE}, we have
%	\begin{equation}\label{eq:NewtonStepAnother}
%	0 = P'_{X_{i-1}}(X_{i} - X_{i-1}) + P(X_{i-1}),
%	\end{equation}
%	and clearly
%	\begin{equation}
%	X_{i} - X_{i-1} = \tXi - \tilde{X}_{i-1}.
%	\end{equation}
%	If we subtract \eqref{eq:NewtonStepAnother} from \eqref{eq:Taylor} 
%	and substitute \eqref{eq:TaylorMPE3}, we obtain
%	\begin{align*}
%	P(X_{i}) & = P(S) + P'_{S}(\tXi) - P(X_{i-1}) - P'_{X_{i-1}}(\tXi - \tilde{X}_{i-1}) + O(\|\tXi\|^{2}) \\
%	& = P'_{S}(\tXi) - P'_{X_{i-1}}(\tXi) + O(\| \tXi \|^{2}) + O(\|\tilde{X}_{i-1}\|^{2}).
%	\end{align*}
%	Putting $S = X_{i-1} - \tilde{X}_{i-1}$ in the previous equality,
%	\begin{align*}
%	P(X_{i}) & = \sum_{k=1}^{n}\sum_{l=0}^{k-1}A_{k}(X_{i-1} - \tilde{X}_{i-1})^{l}\tXi(X_{i-1} - \tilde{X}_{i-1})^{k-l-1} \\
%	& \qquad\qquad\qquad\qquad - P'_{X_{i-1}}(\tXi)	+ O(\| \tXi \|^{2}) + O(\|\tilde{X}_{i-1}\|^{2})\\
%	& = \sum_{k=1}^{n}\sum_{l=0}^{k-1}A_{k}X_{i-1}^{l}\tXi X_{i-1}^{k-l-1} + O(\|\tXi\|\|\tilde{X}_{i-1}\|)\\
%	& \qquad\qquad\qquad\qquad - P'_{X_{i-1}}(\tXi)	+ O(\| \tXi \|^{2}) + O(\|\tilde{X}_{i-1}\|^{2})\\
%	& = P'_{X_{i-1}}(\tXi)+O(\|\tXi\|\|\tilde{X}_{i-1}\|)- P'_{X_{i-1}}(\tXi)+O(\| \tXi \|^{2}) + O(\|\tilde{X}_{i-1}\|^{2})\\
%	& = O(\| \tilde{X}_{i-1} \|^{2}) + O(\| \tXi \|\| \tilde{X}_{i-1} \|) + O(\| \tXi \|^{2}).
%	\end{align*}
%	Since $\|\cdot\|$ is a multiplicative matrix norm on $\mathbb{R}^{m\times m}$,
%	we have required result.
%\end{proof}
The equation \eqref{eq:epsMPE} is not same as \eqref{eq:MPE} but similar.
So, Lemma \ref{lem:TaylorLemma} can be proved like \cite[Lemma 3.3]{SeoSeoKim2018}
because it is used just Taylor's Theorem not the condition of coefficient matrices.

\begin{lemma}\label{lem:DominatedPMconvergence}
	For any fixed $\theta > 0$, let
	\begin{equation}
		\mathcal{Q} = \{i \in \NN \cup \{0\}\,|\,\|\pjm(\tilde{X}_{\e,i})\| > \theta \|\pjn(\tilde{X}_{\e,i})\|\}
	\end{equation}
	where $\{X_{\e, i}\}$ is a Newton sequence for \eqref{eq:epsMPE} which converges to $S$,
	and let ${\color{red}c_{1}} > 0$ be the smallest singular value of $\P'_{\e}[S]|_{\M}$.
	Then, for $\e < c_{1}\theta$, there exist an integer $i_{0}$ and a constant $c > 0$ such that
	$\| \tilde{X}_{\e,i}\| \leq c \| \tilde{X}_{\e,i-1} \|^{2}$ for all $i\geq i_{0}$ in $\mathcal{Q}$.
\end{lemma}

\begin{proof}
	Using Taylor's Theorem and the fact that $P'_{\e}[S]\left(\pjn(\tilde{X}_{\e,i})\right) = \e\pjn(\tilde{X}_{\e,i})$, we obtain that
	\begin{align*}
		P_{\e}(X_{\e,i}) &= P_{\e}(S) + P'_{\e}[S](\tilde{X}_{\e,i}) + O(\|\tilde{X}_{\e,i}\|^{2}) \\
		&= \e\pjn(\tilde{X}_{\e,i}) + P'_{\e}[S]\left(\pjm(\tilde{X}_{\e,i})\right) + O(\|\tilde{X}_{\e,i}\|^{2}). \numberthis\label{eq:TaylorMPE}
	\end{align*}
	Since $c_{1} > 0$ is the smallest singular value of $P'_{\e}[S]|_{\M}$, $\left\| P'_{\e}[S]\left(\pjm(\tilde{X}_{\e,i})\right)\right\| \geq c_{1}\|\pjm(\tilde{X}_{\e,i})\|$.
	For $i\in\mathcal{Q}$, we have
	\begin{equation*}
		\|\tilde{X}_{\e,i}\| \leq \left\| \pjm(\tilde{X}_{\e,i})\right\| + \left\| \pjn(\tilde{X}_{\e,i})\right\| < (\theta^{-1} + 1) \left\| \pjm(\tilde{X}_{\e,i})\right\|.
	\end{equation*}
	Thus by \eqref{eq:TaylorMPE},
	\begin{align*}
		\|P_{\e}(X_{\e,i})\| &= \left\| \e\pjn(\tilde{X}_{\e,i}) + P'_{\e}[S]\left(\pjm(\tilde{X}_{\e,i})\right) + O(\|\tilde{X}_{\e,i}\|^{2}) \right\| \\
		&\geq \left\| \e\pjn(\tilde{X}_{\e,i}) + P'_{\e}[S]\left(\pjm(\tilde{X}_{\e,i})\right)\right\| - c_{2}\|\tilde{X}_{\e,i}\|^{2} \\
		&\geq \left\| P'_{\e}[S]\left(\pjm(\tilde{X}_{\e,i})\right)\right\| - \left\| \e\pjn(\tilde{X}_{\e,i}) \right\| - c_{2}\|\tilde{X}_{\e,i}\|^{2} \numberthis \label{eq:NormofhatXi}\\
		&\geq \left(c_{1} - \frac{\e}{\theta}\right)\|\pjm(\tilde{X}_{\e,i})\| - c_{2}\|\tilde{X}_{\e,i}\|^{2} \\
		&\geq \left(c_{1} - \frac{\e}{\theta}\right)(\theta^{-1}+1)^{-1}\|\tilde{X}_{\e,i}\| - c_{2}\|\tilde{X}_{\e,i}\|^{2}.
	\end{align*}
	On the other hand, from Lemma \ref{lem:TaylorLemma}, we have
	\begin{equation}\label{eq:subNormofhatXi}
		\| P_{\e}({X}_{\e,i})\|\leq c_{3}\|\tilde{X}_{\e,i}\|^{2}+c_{4}\|\tilde{X}_{\e,i-1}\|\|\tilde{X}_{\e,i}\|+c_{5}\|\tilde{X}_{\e,i-1}\|^{2}.
	\end{equation}
	With the combination of \eqref{eq:NormofhatXi} and \eqref{eq:subNormofhatXi}, we have
	$$
	\left(c_{1} - \frac{\e}{\theta}\right)(\theta^{-1}+1)^{-1} - c_{2}\|\tilde{X}_{\e,i}\| \leq c_{3}\|\tilde{X}_{\e,i}\| + c_{4}\|\tilde{X}_{\e,i-1}\| + c_{5}\frac{\|\tilde{X}_{\e,i-1}\|^{2}}{\|\tilde{X}_{\e,i}\|}.
	$$
	Since $\tilde{X}_{\e, i}$ converges to $0$, we can find an $i_{0}$ such that $\|\tilde{X}_{i}\|\leq c\|\tilde{X}_{i-1}\|^{2}$
	for all $i\geq i_{0}$.
\end{proof}

\hrulefill

From \cite{DeckerKelley1985}, we note that $P_{\e}(X) = 0$ is the bifurcation problem where $\e$ is a chosen continuation parameter and the solution arc $S_{\e}$ is a constant arc.
Specially, for $\e = 0$, the MPE \eqref{eq:epsMPE} becomes a singular problem which can be applied by the following theorem refer to \cite[Theorem 1.1]{Kelley1986}.

\begin{theorem}\label{thm:SingularConvergence}
	Let $\pjn (P_{0}''[S](N, N))$ be nonzero and let
	\begin{equation}\label{eq:Wset}
	W(\rho, \theta) = \left\{ X \left|\,0 < \|\tilde{X}\| < \rho, ~\|\pjm(\tilde{X})\| \leq \theta \|\pjn(\tilde{X})\|\right.\right\}.
	\end{equation}
	If $X_{0, 0}\in W(\rho_{0}, \theta_{0})$ for $\rho_{0},\theta_{0}$ sufficiently small, then the Newton sequence $\{X_{0, i}\}$ is well defined and $\| P'_{0}[X_{0,i}]^{-1}\|\leq c\|\tilde{X}_{0,i}\|^{-1}$ for all $i\geq1$ and some constant $c>0$. Moreover,
	\begin{equation}
	\lim_{i \rightarrow \infty}\frac{\|\tilde{X}_{0,i+1}\|}{\|\tilde{X}_{0,i}\|} = \frac{1}{2},
	\qquad \lim_{i \rightarrow \infty}\frac{\|\pjm(\tilde{X}_{0,i})\|}{\|\pjn(\tilde{X}_{0,i})\|^{2}}=0.
	\end{equation}
\end{theorem}

Defining
\begin{align*}
\l N &= \pjn P''_{\e}[S](N, N),\\
\nu N &= \pjn \tilde{X},\numberthis\\
s &= {\rm sign}(\e\l),
\end{align*}
we consider the conical region
\begin{equation}\label{eq:Wsset}
W_{s}(\rho, \theta) = \left\{ X \left|\,0 < \|\tilde{X}\| < \rho, ~\|\pjm(\tilde{X})\| \leq \theta \|\pjn(\tilde{X})\|, ~{\rm sign}(\nu) = s \right.\right\}.
\end{equation}
Then, we can apply the following theorem to the problem for $\e > 0$.

\begin{theorem}{\rm (cf. \cite[Theorem 3.22]{DeckerKelley1985})}\label{thm:NealySingularConvergence}
	Assume $\l \neq 0$, and $\e > 0$.
	Then, there are continuous functions $\rho = \rho(\e)$, $\theta = \theta(\e)$, monotonically increasing as $\e \downarrow 0$ such that if $X_{\e,0} \in W_{s}(\rho, \theta)$ then $P'_{\e}[X_{\e,0}]^{-1}$ exists and all subsequent Newton iterates remain in this set and converges to $S$. Further,
	\begin{equation}
	\|\pjn\tilde{X}_{\e,i+1}\| < \tfrac{3}{4}\|\pjn\tilde{X}_{\e,i}\|, \qquad \|\pjm(\tilde{X}_{\e,i+1})\| \leq K\|\tilde{X}_{\e,i}\|^{2},
	\end{equation}
	for $i \in \NN \cup \{0\}$, some $K > 0$, and $\rho(\e)$, $\theta(\e)$ may be chosen such that $\rho(0) = \rho_{0}$, $\theta(0) = \theta_{0}$ where $\rho_{0}$, $\theta_{0}$ are values for which the conclusions of Theorem \ref{thm:SingularConvergence} hold.
\end{theorem}

\hrulefill

{\color{gray}
\begin{corollary}
	Assume that, for given $\theta>0$, $\|\pjm(\tXi)\|>\theta\|\pjn(\tXi)\|$
	for all $i$ large enough. Then $X_{i}\rightarrow S$ quadratically.
\end{corollary}


When $P'_{S}$ is singular practically the Newton sequence
converges linearly, according to the corollary we conclude that \textit{the
	error will generally be dominated by its $\N$ component}\cite{Guo1998M}.
From Lemmas \ref{lem:BisInvertible} and \ref{lem:DominatedPMconvergence}
we have the following main theorem.

\begin{theorem}\label{thm:LinearConvergence1over2}
	If $-\P'_{S}$ is a singular $M$-matrix and the convergence rate
	of the Newton sequence $\{X_{i}\}$ in Theorem \ref{thm:ConvergenceNewtonMPE}
	is not quadratic, then $\| P_{X_{i}}^{\prime -1}\|\leq c\| \tXi\|^{-1}$
	for all $i \geq 1$ and some constant $c > 0$. Moreover,
	$$
	\lim_{i \rightarrow \infty} \frac{\|\tXii\|}{\|\tXi\|} = \frac{1}{2},
	\qquad \lim_{i\rightarrow\infty} \frac{\|\pjm(\tXi)\|}{\|\pjn(\tXi)\|^{2}}=0.
	$$
	
\end{theorem}
}

\section{Advanced Line Search}

From the previous section, we see that $\pjm(\tilde{X}_{i})$ is almost terminated more than $\pjn(\tilde{X}_{i})$ for the Newton sequence $\{X_{i}\}$
which converges to $S$ and is from a nearly singular problem.
It means that $\{\tilde{X}_{i}\}_{i \geq i_{0}}$ lies on near one-dimensional subspace $\N$ for some nonnegative integer $i_{0}$.
So, with just a proper scalar multiplication, we can accelerate Newton's method as
\begin{equation}\label{eq:accelNewton}
	X_{i+1} = X_{i} - \a_{i} P'[X_{i}]^{-1}(P(X_{i})), \quad i = 0, 1, 2, \ldots.
\end{equation}
As introduced in Section \ref{sec:intro}, the proper scalar $\a_{i}$ is suggested for specific singular problems in \cite{Guo1998M, SeoSeoKim2018}.
If $\|\pjm(\tilde{X}_{i})\| < \e \|\pjn(\tilde{X}_{i})\|$ holds for small $\e > 0$, then $\a_{i} = 2$ is the proper scalar.
But, for nearly singular problems, we cannot guarantee that $\a_{i} = 2$ is the proper.
So, for nearly singular problems, we suggest the way finding the proper scalar $\a_{i}$ based on exact line searches in \cite{Seo2008}.

For given initial guess $X_{0}$, Newton's method with exact line searches is defined by
\begin{equation}
	\begin{cases}
		X_{i+1} = X_{i} + \a_{i} H_{i},\\
		\a_{i} = \underset{1 \leq \a \leq 2}{\rm argmin}\,\|P(X_{i} + \a H_{i})\|^{2},
	\end{cases}
\end{equation}
where $H_{i} = -P'[X_{i}](P(X_{i}))$.
Thus, finding $\a_{i}$ is equivalent to finding the minimum point in the interval $[1, 2]$ of the $2n$-degree polynomial
\begin{align*}
	p_{i}(t) &= \|P(X_{i} + t H_{i})\|^{2} \\
	&= {\rm tr} \left((P(X_{i} + tH_{i})^{T}(P(X_{i} + tH_{i})\right) \numberthis \label{eq:pit}\\
	&= {\rm tr} \left(\left(\sum_{k=0}^{n}A_{k}(X_{i} + tH_{i})^{k}\right)^{T} \sum_{k=0}^{n}A_{k}(X_{i} + tH_{i})^{k}\right).
\end{align*}

To expand \eqref{eq:pit}, let a function $\tau_{(X,H)} : \{0, 1\} \rightarrow \{X, H\}$ be given by $\tau_{(X,H)}(0) = X$ and $\tau_{(X,H)}(1) = H$,
and let $\mathfrak{U}_{k,l} = \left\{\left. {\bf v} \in \{0, 1\}^{k} \, \right| \, {\bf 1}^{T}{\bf v} = l \right\}$.
Then, for $0 \leq l \leq k$, we define
\begin{equation}
	\Phi_{(X, H)}(k, l) = \left\{\begin{array}{ll}\dtyl\sum_{{\bf v} \in \mathfrak{U}_{k,l}} \prod_{j=1}^{k} \tau_{(X,H)}(v_{j})&\textrm{, for }k \neq 0,\\&\\I_{m}&\textrm{, for }k = 0,\end{array}\right.
\end{equation}
where ${\bf v} = \bmatrix{v_{1}&v_{2}&\cdots&v_{k}}^{T}$.
Then, it is easy to verify that
\begin{equation}
	\Phi_{(X, tH)}(k,l) = t^{l}\Phi_{(X, H)}(k,l),\quad\textrm{and}\quad \left(\Phi_{(X, H)}(k,l)\right)^{T} = \Phi_{(X^{T}, H^{T})}(k,l).
\end{equation}
Moreover, 
\begin{align*}
	P(X_{i} + tH_{i}) &= \sum_{k=0}^{n}A_{k}(X_{i} + tH_{i})^{k} \\
	&= \sum_{k=0}^{n}A_{k} \sum_{l=0}^{k} \Phi_{(X_{i}, tH_{i})}(k, l)\\
	&= \sum_{k=0}^{n}\sum_{l=0}^{k} t^{l} A_{k}\Phi_{(X_{i}, H_{i})}(k, l).
\end{align*}
Thus, 
\begin{align*}
	p_{i}(t) 
	&= \mathrm{tr}\left(\left(\sum_{r=0}^{n}A_{r}(X_{i} + tH_{i})^{r}\right)^{T} \sum_{k=0}^{n}A_{k}(X_{i} + tH_{i})^{k}\right)\\
	&= \mathrm{tr}\left(\left(\sum_{r=0}^{n}\sum_{s=0}^{r} t^{s} A_{r}\Phi_{(X_{i}, H_{i})}(r, s)\right)^{T}\left(\sum_{k=0}^{n}\sum_{l=0}^{k} t^{l} A_{k}\Phi_{(X_{i}, H_{i})}(k, l)\right)\right)\\
	&= \mathrm{tr}\left(\left(\sum_{r=0}^{n}\sum_{s=0}^{r} t^{s} \Phi_{(X_{i}^{T}, H_{i}^{T})}(r, s)A_{r}^{T}\right) \left(\sum_{k=0}^{n}\sum_{l=0}^{k} t^{l} A_{k}\Phi_{(X_{i}, H_{i})}(k, l)\right)\right)\\
	&= \sum_{r=0}^{n}\sum_{s=0}^{r}\sum_{k=0}^{n}\sum_{l=0}^{k} t^{s+l} \mathrm{tr}\left(\Phi_{(X_{i}^{T}, H_{i}^{T})}(r, s)A_{r}^{T} A_{k}\Phi_{(X_{i}, H_{i})}(k, l)\right).
\end{align*}
To calculate $\Phi_{(X_{i}, H_{i})}(k,l)$, we need $\binom{k}{l}(k - 1)$-times of matrix multiplications unless $k = 0$.
Hence, there are
\begin{align*}
	&\sum_{r=1}^{n}\sum_{s=0}^{r}\sum_{k=1}^{n}\sum_{l=0}^{k} \left( \binom{r}{s}(r-1) + \binom{k}{l}(k-1) + 3 \right)\\
	&\quad=\left(2^{n+2}(n-2)+8\right)\left(\frac{(n+1)(n+2)}{2}-1\right) + 3\left(\frac{(n+1)(n+2)}{2}-1\right)^{2}=O(2^{n}n)
\end{align*}
of matrix multiplications to obtain all coefficients of $p_{i}$.


\section{Note}

\begin{align*}
	X_{i+1} &= X_{i} + H_{i} \\
	P'[X_{i}](H_{i}) &= -P(X_{i})
\end{align*}

\begin{align*}
	\P'[X_{i}] &= \sum_{k=1}^{n} \sum_{l=0}^{k-1} (X_{i}^{k-l-1})^{T} \otimes A_{k}X_{i}^{l}\\
	&\Downarrow\\
	{\rm mxmus}&= \sum_{k=2}^{n} \sum_{l=0}^{k-2} \left((k-l-2) + l + m \right)\\
	&= \sum_{k=2}^{n} \sum_{l=0}^{k-2} \left(k - 2 + m \right)\\
	&= \frac16 n(n+1)(2n + 1) + \frac12 n(n+1)(m-3) + n(2-m)\\
	&= O(n^{3}) + O(n^{2}m)
\end{align*}

\begin{align*}
	\P'[X_{i}]h_{i} &= -\vec(P(X_{i}))\\
	&\Downarrow\\
	{\rm flops}&= O(m^{6})\\
	{\rm mxmus}&= O(m^{3})
\end{align*}

{\color{gray}\begin{align*}
	P(X_{i}) &= \sum_{k=0}^{n} A_{k}X_{i}^{k}\\
	&\Downarrow\\
	{\rm mxmus}&= \sum_{k=0}^{n} k\\
	&= O(n^{2})
\end{align*}}

a
%\begin{align*}
%	p_{i}(t) 
%	&= \mathrm{tr}\left(\left(\sum_{k=0}^{n}A_{k}(X_{i} + tH_{i})^{k}\right)^{T} \sum_{k=0}^{n}A_{k}(X_{i} + tH_{i})^{k}\right)\\
%	&= \mathrm{tr}\left(\left(\sum_{l=0}^{n}\sum_{k=l}^{n} t^{l} A_{k}\Phi_{(X_{i}, H_{i})}(k, l)\right)^{T}\left(\sum_{l=0}^{n}\sum_{k=l}^{n} t^{l} A_{k}\Phi_{(X_{i}, H_{i})}(k, l)\right)\right)\\
%	&= \mathrm{tr}\left(\left(\sum_{l=0}^{n}\sum_{k=l}^{n} t^{l} \Phi_{(X_{i}^{T}, H_{i}^{T})}(k, l)A_{k}^{T}\right) \left(\sum_{l=0}^{n}\sum_{k=l}^{n} t^{l} A_{k}\Phi_{(X_{i}, H_{i})}(k, l)\right)\right)\\
%	&= \mathrm{tr}\left(\sum_{s=0}^{n}\sum_{r=s}^{n}\sum_{l=0}^{n}\sum_{k=l}^{n} t^{s+l} \Phi_{(X_{i}^{T}, H_{i}^{T})}(r, s)A_{r}^{T} A_{k}\Phi_{(X_{i}, H_{i})}(k, l)\right)\\
%	&= \sum_{s=0}^{n}\sum_{r=s}^{n}\sum_{l=0}^{n}\sum_{k=l}^{n} t^{s+l} \mathrm{tr}\left(\Phi_{(X_{i}^{T}, H_{i}^{T})}(r, s)A_{r}^{T} A_{k}\Phi_{(X_{i}, H_{i})}(k, l)\right)\\
%	&= \sum_{s=0}^{n}\sum_{l=0}^{n} t^{s+l} \sum_{r=s}^{n}\sum_{k=l}^{n} \mathrm{tr}\left(\Phi_{(X_{i}^{T}, H_{i}^{T})}(r, s)A_{r}^{T} A_{k}\Phi_{(X_{i}, H_{i})}(k, l)\right)\\
%	&= \sum_{r=0}^{n}\sum_{s=0}^{r}\sum_{k=0}^{n}\sum_{l=0}^{k} t^{s+l} \mathrm{tr}\left(\Phi_{(X_{i}^{T}, H_{i}^{T})}(r, s)A_{r}^{T} A_{k}\Phi_{(X_{i}, H_{i})}(k, l)\right).
%\end{align*}
%
%\begin{align*}
%	&\sum_{r=1}^{n}\sum_{s=0}^{r}\sum_{k=1}^{n}\sum_{l=0}^{k} \left( \binom{r}{s}(r-1) + \binom{k}{l}(k-1) + 3 \right)\\
%	&=\sum_{r=1}^{n}\sum_{s=0}^{r}\sum_{k=1}^{n}\sum_{l=0}^{k} \binom{r}{s}(r-1) + \sum_{r=1}^{n}\sum_{s=0}^{r}\sum_{k=1}^{n}\sum_{l=0}^{k} \binom{k}{l}(k-1) + \sum_{r=1}^{n}\sum_{s=0}^{r}\sum_{k=1}^{n}\sum_{l=0}^{k} 3\\
%	&=\sum_{r=1}^{n}\sum_{s=0}^{r}\sum_{k=1}^{n}\sum_{l=0}^{k} \binom{r}{s}(r-1) + \sum_{r=1}^{n}\sum_{s=0}^{r}\sum_{k=1}^{n}\sum_{l=0}^{k} \binom{k}{l}(k-1) + 3\left(\frac{(n+1)(n+2)}{2}-1\right)^{2}\\
%	&=\sum_{r=1}^{n}\sum_{s=0}^{r}\sum_{k=1}^{n}\sum_{l=0}^{k} \binom{r}{s}(r-1) + \left(2^{n+1}(n-2)+4\right)\left(\frac{(n+1)(n+2)}{2}-1\right) + 3\left(\frac{(n+1)(n+2)}{2}-1\right)^{2}\\
%	&=\left(2^{n+2}(n-2)+8\right)\left(\frac{(n+1)(n+2)}{2}-1\right) + 3\left(\frac{(n+1)(n+2)}{2}-1\right)^{2}
%\end{align*}
%
%Now, we consider an MPE with other coefficients given by
%\begin{equation}\label{eq:eMPE}
%P_{\e}(X) = (A_{0} + \e S) + (A_{1} - \e I)X + \sum_{k=2}^{n}A_{k}X^{k},
%\end{equation}
%where $\e \geq 0$ and $S$ is the minimal positive solution in Theorem \ref{thm:ConvergenceNewtonMPE}.
%Then, \eqref{eq:eMPE} also satisfies Assumption \ref{ass:MPE}.
%Moreover, for $Y_{1} \leq Y_{2} \in L[0,S]:=\{Y \in \RR^{\mbym} ~|~ 0 \leq Y \leq S\}$,
%\begin{align*}
%	(P'_{\e}[Y_{2}] - P'_{\e}[Y_{1}])(Y_{2} - Y_{1}) &= \sum_{k=2}^{n} \sum_{l=0}^{k-1} A_{k}(Y_{2}^{l}(Y_{2}-Y_{1})Y_{2}^{k-l-1} - Y_{1}^{l}(Y_{2}-Y_{1})Y_{1}^{k-l-1})\\
%	&\qquad + (A_{1}-\e I)(Y_{2}-Y_{1}) - (A_{1}-\e I)(Y_{2}-Y_{1})\geq 0,
%\end{align*}
%i.e., $P_{\e}$ is order-convex on $L[0,S]$.
%Since $P_{\e}(0) = A_{0} + \e S \geq 0$ and $P_{\e}(S) = 0$, $S$ is the unique solution for $P_{\e}(X) = 0$ on $L[0,S]$ by \cite[Monotone Newton Theorem]{Ortega2000}.
%Thus, $P_{\e}(X) = 0$ has $S$ as the minimal nonnegative solution although the coefficients of $P_{\e}$ do not satisfy \eqref{eq:SuffMPE}, always.



\bibliographystyle{plain}
\bibliography{SHSeo}

\end{document}